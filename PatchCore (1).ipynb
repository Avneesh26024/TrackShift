{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#⚙️ Stage 0: Configuration"
      ],
      "metadata": {
        "id": "ma_QXYqAoncc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-xHLzs6njLe",
        "outputId": "b032b4f9-776c-49be-e849-27450aeee1b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Configuration loaded.\n",
            "   Golden Image: /content/Golden_Image.jpg\n",
            "   Current Image: /content/Current_Image.jpg\n",
            "   Benchmark: \n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import json\n",
        "import torch\n",
        "from transformers import AutoImageProcessor, AutoModel, pipeline\n",
        "from PIL import Image\n",
        "from scipy.ndimage import maximum_filter\n",
        "import os # Import the 'os' module to check if files exist\n",
        "\n",
        "# --- 1. Define Input File Paths ---\n",
        "GOLDEN_IMAGE_PATH = \"/content/Golden_Image.jpg\"\n",
        "CURRENT_IMAGE_PATH = \"/content/Current_Image.jpg\"\n",
        "\n",
        "# --- Set benchmark path (or leave as \"\" if you don't have one) ---\n",
        "BENCHMARK_IMAGE_PATH =  \"\" #\"/content/BenchMark_Crack.jpg\"\n",
        "\n",
        "# --- 2. Define Output File Paths ---\n",
        "OUTPUT_OVERLAY_PATH = \"/content/sam2_output_overlay.png\"\n",
        "OUTPUT_REPORT_PATH = \"/content/report.json\"\n",
        "OUTPUT_VALIDATION_PATH = \"/content/validation_diff.jpg\"\n",
        "\n",
        "print(\"✅ Configuration loaded.\")\n",
        "print(f\"   Golden Image: {GOLDEN_IMAGE_PATH}\")\n",
        "print(f\"   Current Image: {CURRENT_IMAGE_PATH}\")\n",
        "print(f\"   Benchmark: {BENCHMARK_IMAGE_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import cv2\n",
        "# import numpy as np\n",
        "# import json\n",
        "# import torch\n",
        "# from transformers import AutoImageProcessor, AutoModel, pipeline\n",
        "# from PIL import Image\n",
        "# from scipy.ndimage import maximum_filter\n",
        "# import os # Import the 'os' module to check if files exist\n",
        "\n",
        "# # --- 1. Define Input File Paths ---\n",
        "# # --- ⬇️ CORRECTION: Use the correct \"before\" and \"after\" pair ⬇️ ---\n",
        "# GOLDEN_IMAGE_PATH = \"/content/PCB_3_REF.jpg\"\n",
        "# CURRENT_IMAGE_PATH = \"/content/PCB_4_CURR.jpg\"\n",
        "\n",
        "\n",
        "# # --- Set benchmark path (or leave as \"\" if you don't have one) ---\n",
        "# BENCHMARK_IMAGE_PATH =  \"\" # No benchmark for this, so we leave it empty\n",
        "\n",
        "# # --- 2. Define Output File Paths ---\n",
        "# OUTPUT_OVERLAY_PATH = \"/content/sam2_output_overlay.png\"\n",
        "# OUTPUT_REPORT_PATH = \"/content/report.json\"\n",
        "# OUTPUT_VALIDATION_PATH = \"/content/validation_diff.jpg\"\n",
        "\n",
        "# print(\"✅ Configuration loaded.\")\n",
        "# print(f\"   Golden Image: {GOLDEN_IMAGE_PATH}\")\n",
        "# print(f\"   Current Image: {CURRENT_IMAGE_PATH}\")\n",
        "# print(f\"   Benchmark: {BENCHMARK_IMAGE_PATH}\")"
      ],
      "metadata": {
        "id": "jX-h4WZ5sFp-"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1️⃣ Stage 1: ALIGN"
      ],
      "metadata": {
        "id": "AS5HkgDsowq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load reference and current images\n",
        "ref = cv2.imread(GOLDEN_IMAGE_PATH, cv2.IMREAD_COLOR)\n",
        "curr = cv2.imread(CURRENT_IMAGE_PATH, cv2.IMREAD_COLOR)\n",
        "\n",
        "# Detect keypoints and descriptors\n",
        "sift = cv2.SIFT_create()\n",
        "kp1, des1 = sift.detectAndCompute(ref, None)\n",
        "kp2, des2 = sift.detectAndCompute(curr, None)\n",
        "\n",
        "# Match keypoints\n",
        "bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)\n",
        "matches = bf.match(des1, des2)\n",
        "matches = sorted(matches, key=lambda x: x.distance)\n",
        "\n",
        "# Compute homography and warp\n",
        "src_pts = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1,1,2)\n",
        "dst_pts = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1,1,2)\n",
        "H, _ = cv2.findHomography(dst_pts, src_pts, cv2.RANSAC, 5.0)\n",
        "aligned = cv2.warpPerspective(curr, H, (ref.shape[1], ref.shape[0]))\n",
        "\n",
        "print(\"✅ Stage 1 complete. Image aligned.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWpYFX8Ho0ip",
        "outputId": "91fbd7fa-6bd9-411f-b85e-56dcbc5ef82d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Stage 1 complete. Image aligned.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2️⃣ Stage 2: DETECT (DINOv2)"
      ],
      "metadata": {
        "id": "BXU1ggA2o1No"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Load Model ---\n",
        "processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base\")\n",
        "model = AutoModel.from_pretrained(\"facebook/dinov2-base\").to(\"cuda\")\n",
        "\n",
        "# --- Process Images ---\n",
        "img_ref = Image.open(GOLDEN_IMAGE_PATH)\n",
        "img_aligned = Image.fromarray(cv2.cvtColor(aligned, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "inputs_ref = processor(img_ref, return_tensors=\"pt\").to(\"cuda\")\n",
        "inputs_curr = processor(img_aligned, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# --- Compute Features ---\n",
        "with torch.no_grad():\n",
        "    feats_ref = model(**inputs_ref).last_hidden_state\n",
        "    feats_curr = model(**inputs_curr).last_hidden_state\n",
        "\n",
        "# --- Exclude the [CLS] token ---\n",
        "feats_ref_patches = feats_ref[:, 1:, :]\n",
        "feats_curr_patches = feats_curr[:, 1:, :]\n",
        "\n",
        "# --- Compute Cosine Similarity ---\n",
        "cos_sim = torch.nn.functional.cosine_similarity(feats_ref_patches, feats_curr_patches, dim=-1)\n",
        "\n",
        "# --- Reshape to 16x16 ---\n",
        "heatmap_small = (1 - cos_sim[0]).reshape(16, 16).cpu().numpy()\n",
        "\n",
        "# --- Resize heatmap ---\n",
        "heatmap = cv2.resize(heatmap_small,\n",
        "                     (aligned.shape[1], aligned.shape[0]),\n",
        "                     interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "print(\"✅ Stage 2 complete. Full-size heatmap generated.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afqeCs8Fo4-I",
        "outputId": "9dd4faed-94e1-4c93-c6c4-b2e55ebff8b0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Stage 2 complete. Full-size heatmap generated.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3️⃣ Stage 3: SEGMENT (SAM 2)"
      ],
      "metadata": {
        "id": "5o-rnlXlo5ir"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # --- Initialize Model ---\n",
        "# mask_generator = pipeline(\n",
        "#     \"mask-generation\",\n",
        "#     model=\"facebook/sam2-hiera-large\",\n",
        "#     device_map=\"cuda\"\n",
        "# )\n",
        "\n",
        "# # --- Convert Image ---\n",
        "# image_pil = Image.fromarray(cv2.cvtColor(aligned, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "# # --- IMPROVED PROMPT GENERATION (WITH BLURRING) ---\n",
        "\n",
        "# # 1. Normalize heatmap\n",
        "# heatmap_norm = cv2.normalize(heatmap, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
        "\n",
        "# # 2. De-noise the heatmap to remove speckles\n",
        "# heatmap_blurred = cv2.GaussianBlur(heatmap_norm, (11, 11), 0)\n",
        "\n",
        "# # 3. Find ALL local peaks on the *blurred* heatmap\n",
        "# footprint = np.ones((20, 20)) # Neighborhood size\n",
        "# local_max = maximum_filter(heatmap_blurred, footprint=footprint)\n",
        "# peaks_mask = (heatmap_blurred == local_max)\n",
        "\n",
        "# # 4. Apply a threshold to the peaks (adjust '40' if needed)\n",
        "# peaks_mask = np.logical_and(peaks_mask, heatmap_blurred > 40)\n",
        "\n",
        "# # 5. Get the (y, x) coordinates of these peaks\n",
        "# y_coords, x_coords = np.where(peaks_mask)\n",
        "\n",
        "# # 6. Format them as [x, y] points for SAM\n",
        "# points = [[x, y] for x, y in zip(x_coords, y_coords)]\n",
        "# labels = [1] * len(points) # '1' means foreground\n",
        "\n",
        "# if not points:\n",
        "#     print(\"⚠️ No peaks found above the threshold. Try lowering it.\")\n",
        "# else:\n",
        "#     print(f\"✅ Found {len(points)} precise peak prompts from heatmap.\")\n",
        "\n",
        "# # --- Generate Masks using POINT prompts ---\n",
        "# if points:\n",
        "#     results = mask_generator(\n",
        "#         image_pil,\n",
        "#         points=[points], # Note the extra list wrapping\n",
        "#         labels=[labels],\n",
        "#         boxes=None\n",
        "#     )\n",
        "#     masks = results[\"masks\"]\n",
        "# else:\n",
        "#     masks = []\n",
        "\n",
        "# print(f\"✅ SAM 2 generated {len(masks)} masks from point prompts.\")\n",
        "\n",
        "# # --- Visualization Loop ---\n",
        "# overlay = aligned.copy()\n",
        "# color = [0, 0, 255] # Red (BGR)\n",
        "# target_dsize = (aligned.shape[1], aligned.shape[0])\n",
        "\n",
        "# for m in masks:\n",
        "#     m_np = np.array(m).astype(np.uint8) * 255\n",
        "#     m_resized = cv2.resize(m_np, target_dsize, interpolation=cv2.INTER_NEAREST)\n",
        "#     m_bin = m_resized > 0\n",
        "#     overlay[m_bin] = color\n",
        "\n",
        "# # --- Save as lossless PNG ---\n",
        "# cv2.imwrite(OUTPUT_OVERLAY_PATH, overlay)\n",
        "# print(f\"✅ Segmentation complete — saved overlay to {OUTPUT_OVERLAY_PATH}\")"
      ],
      "metadata": {
        "id": "DYWGMYU0o-BA"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Improved Stage-3"
      ],
      "metadata": {
        "id": "7XGPPGOFtu1O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import pipeline\n",
        "# import torch\n",
        "# from PIL import Image\n",
        "# import numpy as np\n",
        "# import cv2\n",
        "# from scipy.ndimage import maximum_filter\n",
        "\n",
        "# # --- Initialize Model ---\n",
        "# mask_generator = pipeline(\n",
        "#     \"mask-generation\",\n",
        "#     model=\"facebook/sam2-hiera-large\",\n",
        "#     device_map=\"cuda\"\n",
        "# )\n",
        "\n",
        "# # --- Convert Image ---\n",
        "# image_pil = Image.fromarray(cv2.cvtColor(aligned, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "# # --- IMPROVED PROMPT GENERATION (WITH BLURRING) ---\n",
        "\n",
        "# # 1. Normalize heatmap\n",
        "# heatmap_norm = cv2.normalize(heatmap, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
        "\n",
        "# # 2. De-noise the heatmap\n",
        "# heatmap_blurred = cv2.GaussianBlur(heatmap_norm, (11, 11), 0)\n",
        "\n",
        "# # 3. Find ALL local peaks on the *blurred* heatmap\n",
        "# footprint = np.ones((20, 20)) # Neighborhood size\n",
        "# local_max = maximum_filter(heatmap_blurred, footprint=footprint)\n",
        "# peaks_mask = (heatmap_blurred == local_max)\n",
        "\n",
        "# # 4. Apply a threshold to the peaks\n",
        "# # --- ⬇️ CORRECTION: Increased threshold to filter out noise ⬇️ ---\n",
        "# peaks_mask = np.logical_and(peaks_mask, heatmap_blurred > 80)\n",
        "\n",
        "# # 5. Get the (y, x) coordinates of these peaks\n",
        "# y_coords, x_coords = np.where(peaks_mask)\n",
        "\n",
        "# # 6. Format them as [x, y] points for SAM\n",
        "# points = [[x, y] for x, y in zip(x_coords, y_coords)]\n",
        "# labels = [1] * len(points) # '1' means foreground\n",
        "\n",
        "# if not points:\n",
        "#     print(\"✅ Found 0 peaks. This is the correct result for identical images.\")\n",
        "# else:\n",
        "#     print(f\"✅ Found {len(points)} precise peak prompts from heatmap.\")\n",
        "\n",
        "# # --- Generate Masks using POINT prompts ---\n",
        "# if points:\n",
        "#     results = mask_generator(\n",
        "#         image_pil,\n",
        "#         points=[points], # Note the extra list wrapping\n",
        "#         labels=[labels],\n",
        "#         boxes=None\n",
        "#     )\n",
        "#     masks = results[\"masks\"]\n",
        "# else:\n",
        "#     masks = []\n",
        "\n",
        "# print(f\"✅ SAM 2 generated {len(masks)} masks from point prompts.\")\n",
        "\n",
        "# # --- Visualization Loop ---\n",
        "# overlay = aligned.copy()\n",
        "# color = [0, 0, 255] # Red (BGR)\n",
        "# target_dsize = (aligned.shape[1], aligned.shape[0])\n",
        "\n",
        "# for m in masks:\n",
        "#     m_np = np.array(m).astype(np.uint8) * 255\n",
        "#     m_resized = cv2.resize(m_np, target_dsize, interpolation=cv2.INTER_NEAREST)\n",
        "#     m_bin = m_resized > 0\n",
        "#     overlay[m_bin] = color\n",
        "\n",
        "# # --- Save as lossless PNG ---\n",
        "# cv2.imwrite(OUTPUT_OVERLAY_PATH, overlay)\n",
        "# print(f\"✅ Segmentation complete — saved overlay to {OUTPUT_OVERLAY_PATH}\")"
      ],
      "metadata": {
        "id": "Wwmh31iitxUi"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Higher Threshold Stage-3\n"
      ],
      "metadata": {
        "id": "Nbppev1LuZmo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import pipeline\n",
        "# import torch\n",
        "# from PIL import Image\n",
        "# import numpy as np\n",
        "# import cv2\n",
        "# from scipy.ndimage import maximum_filter\n",
        "\n",
        "# # --- Initialize Model ---\n",
        "# mask_generator = pipeline(\n",
        "#     \"mask-generation\",\n",
        "#     model=\"facebook/sam2-hiera-large\",\n",
        "#     device_map=\"cuda\"\n",
        "# )\n",
        "\n",
        "# # --- Convert Image ---\n",
        "# image_pil = Image.fromarray(cv2.cvtColor(aligned, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "# # --- IMPROVED PROMPT GENERATION (WITH BLURRING) ---\n",
        "\n",
        "# # 1. Normalize heatmap\n",
        "# heatmap_norm = cv2.normalize(heatmap, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
        "\n",
        "# # 2. De-noise the heatmap\n",
        "# heatmap_blurred = cv2.GaussianBlur(heatmap_norm, (11, 11), 0)\n",
        "\n",
        "# # 3. Find ALL local peaks on the *blurred* heatmap\n",
        "# footprint = np.ones((20, 20)) # Neighborhood size\n",
        "# local_max = maximum_filter(heatmap_blurred, footprint=footprint)\n",
        "# peaks_mask = (heatmap_blurred == local_max)\n",
        "\n",
        "# # 4. Apply a threshold to the peaks\n",
        "# # --- ⬇️ CORRECTION: Drastically Increased threshold ⬇️ ---\n",
        "# peaks_mask = np.logical_and(peaks_mask, heatmap_blurred > 150) # Was 80\n",
        "\n",
        "# # 5. Get the (y, x) coordinates of these peaks\n",
        "# y_coords, x_coords = np.where(peaks_mask)\n",
        "\n",
        "# # 6. Format them as [x, y] points for SAM\n",
        "# points = [[x, y] for x, y in zip(x_coords, y_coords)]\n",
        "# labels = [1] * len(points) # '1' means foreground\n",
        "\n",
        "# if not points:\n",
        "#     print(\"✅ Found 0 peaks. This is the correct result for identical images.\")\n",
        "# else:\n",
        "#     print(f\"✅ Found {len(points)} precise peak prompts from heatmap.\")\n",
        "\n",
        "# # --- Generate Masks using POINT prompts ---\n",
        "# if points:\n",
        "#     results = mask_generator(\n",
        "#         image_pil,\n",
        "#         points=[points], # Note the extra list wrapping\n",
        "#         labels=[labels],\n",
        "#         boxes=None\n",
        "#     )\n",
        "#     masks = results[\"masks\"]\n",
        "# else:\n",
        "#     masks = []\n",
        "\n",
        "# print(f\"✅ SAM 2 generated {len(masks)} masks from point prompts.\")\n",
        "\n",
        "# # --- Visualization Loop ---\n",
        "# overlay = aligned.copy()\n",
        "# color = [0, 0, 255] # Red (BGR)\n",
        "# target_dsize = (aligned.shape[1], aligned.shape[0])\n",
        "\n",
        "# for m in masks:\n",
        "#     m_np = np.array(m).astype(np.uint8) * 255\n",
        "#     m_resized = cv2.resize(m_np, target_dsize, interpolation=cv2.INTER_NEAREST)\n",
        "#     m_bin = m_resized > 0\n",
        "#     overlay[m_bin] = color\n",
        "\n",
        "# # --- Save as lossless PNG ---\n",
        "# cv2.imwrite(OUTPUT_OVERLAY_PATH, overlay)\n",
        "# print(f\"✅ Segmentation complete — saved overlay to {OUTPUT_OVERLAY_PATH}\")"
      ],
      "metadata": {
        "id": "FTM8jj_RucBq"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # --- DEBUG: Save heatmaps for visualization ---\n",
        "# cv2.imwrite(\"/content/DEBUG_heatmap_normalized.png\", heatmap_norm)\n",
        "# cv2.imwrite(\"/content/DEBUG_heatmap_blurred.png\", heatmap_blurred)\n",
        "\n",
        "# print(\"✅ Debug heatmaps saved. Please check:\")\n",
        "# print(\"   - /content/DEBUG_heatmap_normalized.png\")\n",
        "# print(\"   - /content/DEBUG_heatmap_blurred.png\")"
      ],
      "metadata": {
        "id": "wx8Y0p_iudkM"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import cv2\n",
        "from scipy.ndimage import maximum_filter\n",
        "\n",
        "# --- Initialize Model ---\n",
        "mask_generator = pipeline(\n",
        "    \"mask-generation\",\n",
        "    model=\"facebook/sam2-hiera-large\",\n",
        "    device_map=\"cuda\"\n",
        ")\n",
        "\n",
        "# --- Convert Image ---\n",
        "image_pil = Image.fromarray(cv2.cvtColor(aligned, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "# --- IMPROVED PROMPT GENERATION (WITH STRONGER FILTERING) ---\n",
        "\n",
        "# 1. Normalize heatmap\n",
        "heatmap_norm = cv2.normalize(heatmap, None, 0, 255, cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
        "\n",
        "# 2. De-noise the heatmap more aggressively\n",
        "# --- ⬇️ Increased Gaussian Blur Kernel Size ⬇️ ---\n",
        "heatmap_blurred = cv2.GaussianBlur(heatmap_norm, (21, 21), 0) # Was (11, 11)\n",
        "\n",
        "# 3. Find ALL local peaks on the *blurred* heatmap\n",
        "footprint = np.ones((20, 20)) # Neighborhood size (keep as is for now)\n",
        "local_max = maximum_filter(heatmap_blurred, footprint=footprint)\n",
        "peaks_mask = (heatmap_blurred == local_max)\n",
        "\n",
        "# 4. Apply a much higher threshold to the peaks\n",
        "# --- ⬇️ Drastically Increased threshold ⬇️ ---\n",
        "peaks_mask = np.logical_and(peaks_mask, heatmap_blurred > 200) # Was 150\n",
        "\n",
        "# 5. Get the (y, x) coordinates of these peaks\n",
        "y_coords, x_coords = np.where(peaks_mask)\n",
        "\n",
        "# 6. Format them as [x, y] points for SAM\n",
        "points = [[x, y] for x, y in zip(x_coords, y_coords)]\n",
        "labels = [1] * len(points) # '1' means foreground\n",
        "\n",
        "if not points:\n",
        "    # This is the expected output now\n",
        "    print(\"✅ Found 0 peaks. This is the correct result for identical images.\")\n",
        "else:\n",
        "    # If peaks are still found, the noise is incredibly high\n",
        "    print(f\"⚠️ Found {len(points)} peaks even with strong filtering.\")\n",
        "    print(f\"   Consider alternative alignment or comparison methods if noise persists.\")\n",
        "\n",
        "\n",
        "# --- Generate Masks using POINT prompts ---\n",
        "if points:\n",
        "    results = mask_generator(\n",
        "        image_pil,\n",
        "        points=[points], # Note the extra list wrapping\n",
        "        labels=[labels],\n",
        "        boxes=None\n",
        "    )\n",
        "    masks = results[\"masks\"]\n",
        "else:\n",
        "    masks = [] # No points means no masks\n",
        "\n",
        "print(f\"✅ SAM 2 generated {len(masks)} masks from point prompts.\")\n",
        "\n",
        "# --- Visualization Loop ---\n",
        "overlay = aligned.copy()\n",
        "color = [0, 0, 255] # Red (BGR)\n",
        "target_dsize = (aligned.shape[1], aligned.shape[0])\n",
        "\n",
        "# This loop won't run if masks is empty\n",
        "for m in masks:\n",
        "    m_np = np.array(m).astype(np.uint8) * 255\n",
        "    m_resized = cv2.resize(m_np, target_dsize, interpolation=cv2.INTER_NEAREST)\n",
        "    m_bin = m_resized > 0\n",
        "    overlay[m_bin] = color\n",
        "\n",
        "# --- Save as lossless PNG ---\n",
        "cv2.imwrite(OUTPUT_OVERLAY_PATH, overlay)\n",
        "print(f\"✅ Segmentation complete — saved overlay to {OUTPUT_OVERLAY_PATH}\")\n",
        "\n",
        "# --- Also save the heatmaps for debugging ---\n",
        "cv2.imwrite(\"/content/DEBUG_heatmap_normalized.png\", heatmap_norm)\n",
        "cv2.imwrite(\"/content/DEBUG_heatmap_blurred_strong.png\", heatmap_blurred) # New name\n",
        "print(\"✅ Debug heatmaps saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rdiZUIEhJiGU",
        "outputId": "345a13eb-f1a3-4e58-de60-69b9a9b2f81d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ Found 17 peaks even with strong filtering.\n",
            "   Consider alternative alignment or comparison methods if noise persists.\n",
            "✅ SAM 2 generated 122 masks from point prompts.\n",
            "✅ Segmentation complete — saved overlay to /content/sam2_output_overlay.png\n",
            "✅ Debug heatmaps saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4️⃣ Stage 4: REPORT (JSON)"
      ],
      "metadata": {
        "id": "OfxGWwD8pCzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Cast np.int64 to standard python int() ---\n",
        "report_points = [{\"x\": int(p[0]), \"y\": int(p[1])} for p in points]\n",
        "\n",
        "report = {\n",
        "    \"num_differences_found\": len(report_points),\n",
        "    \"prompt_type\": \"points\",\n",
        "    \"differences\": report_points\n",
        "}\n",
        "\n",
        "with open(OUTPUT_REPORT_PATH, \"w\") as f:\n",
        "    json.dump(report, f, indent=2)\n",
        "\n",
        "print(f\"✅ JSON report saved to {OUTPUT_REPORT_PATH}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4Gmc5L1pFDX",
        "outputId": "5f834645-095d-4dc2-d03c-e2045161d988"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ JSON report saved to /content/report.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5️⃣ Stage 5: EVALUATE (IoU)"
      ],
      "metadata": {
        "id": "4VuFDl6MpKDD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"🚀 Starting Stage 5: Benchmark Validation\")\n",
        "\n",
        "# --- 1. Define File Paths from Stage 0 ---\n",
        "ref = cv2.imread(GOLDEN_IMAGE_PATH, cv2.IMREAD_COLOR)\n",
        "predicted_overlay_path = OUTPUT_OVERLAY_PATH\n",
        "benchmark_path = BENCHMARK_IMAGE_PATH\n",
        "validation_output_path = OUTPUT_VALIDATION_PATH\n",
        "report_path = OUTPUT_REPORT_PATH\n",
        "\n",
        "# --- 2. ⬇️ MODIFIED: Check if Benchmark File Exists ⬇️ ---\n",
        "if not os.path.exists(benchmark_path):\n",
        "    print(f\"⚠️ Benchmark file not found at: {benchmark_path}\")\n",
        "    print(f\"   Skipping evaluation. Your output is saved at: {OUTPUT_OVERLAY_PATH}\")\n",
        "\n",
        "else:\n",
        "    # --- 3. Load Images (Benchmark exists, so proceed) ---\n",
        "    print(f\"✅ Benchmark found at {benchmark_path}. Running evaluation...\")\n",
        "    try:\n",
        "        predicted_overlay = cv2.imread(predicted_overlay_path)\n",
        "        if predicted_overlay is None:\n",
        "            raise IOError(f\"Could not load predicted overlay from {predicted_overlay_path}\")\n",
        "\n",
        "        benchmark_img = cv2.imread(benchmark_path, cv2.IMREAD_GRAYSCALE)\n",
        "        if benchmark_img is None:\n",
        "            raise IOError(f\"Could not load benchmark image from {benchmark_path}\")\n",
        "\n",
        "        print(\"✅ Images loaded successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ ERROR: {e}\")\n",
        "        print(f\"Please ensure '{predicted_overlay_path}' and '{benchmark_path}' are valid image files.\")\n",
        "\n",
        "    else:\n",
        "        # --- 4. Resize Benchmark to Match Output ---\n",
        "        target_shape = (ref.shape[1], ref.shape[0]) # (width, height)\n",
        "        if (benchmark_img.shape[1], benchmark_img.shape[0]) != target_shape:\n",
        "            print(f\"Resizing benchmark from {benchmark_img.shape} to {target_shape}\")\n",
        "            benchmark_img = cv2.resize(benchmark_img, target_shape, interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "        # --- 5. Create Boolean Masks ---\n",
        "        predicted_mask = (predicted_overlay[:, :, 2] > 200) & \\\n",
        "                         (predicted_overlay[:, :, 1] < 50) & \\\n",
        "                         (predicted_overlay[:, :, 0] < 50)\n",
        "\n",
        "        # --- CRITICAL BENCHMARK FIX ---\n",
        "        # Assumes benchmark is BLACK crack on WHITE background\n",
        "        ground_truth_mask = benchmark_img < 100\n",
        "\n",
        "        # --- 6. Calculate Metrics (IoU) ---\n",
        "        intersection = np.logical_and(predicted_mask, ground_truth_mask)\n",
        "        union = np.logical_or(predicted_mask, ground_truth_mask)\n",
        "\n",
        "        if np.sum(union) > 0:\n",
        "            iou_score = np.sum(intersection) / np.sum(union)\n",
        "        else:\n",
        "            iou_score = 1.0 if np.sum(intersection) == 0 else 0.0\n",
        "\n",
        "        print(f\"---\" * 10)\n",
        "        print(f\"📊 Validation Score (IoU): {iou_score:.4f}\")\n",
        "        print(f\"---\" * 10)\n",
        "\n",
        "        # --- 7. Create Visual Validation \"Diff\" Image ---\n",
        "        validation_image = ref.copy()\n",
        "        TP_COLOR = [255, 0, 255]  # Magenta (True Positive)\n",
        "        FP_COLOR = [0, 0, 255]    # Red (False Positive)\n",
        "        FN_COLOR = [255, 255, 0]  # Cyan (False Negative)\n",
        "\n",
        "        tp_mask = intersection\n",
        "        fp_mask = np.logical_and(predicted_mask, np.logical_not(ground_truth_mask))\n",
        "        fn_mask = np.logical_and(np.logical_not(predicted_mask), ground_truth_mask)\n",
        "\n",
        "        validation_image[tp_mask] = TP_COLOR\n",
        "        validation_image[fp_mask] = FP_COLOR\n",
        "        validation_image[fn_mask] = FN_COLOR\n",
        "\n",
        "        cv2.imwrite(validation_output_path, validation_image)\n",
        "        print(f\"✅ Validation 'diff' image saved to {validation_output_path}\")\n",
        "\n",
        "        # --- 8. (Optional) Update JSON Report with IoU Score ---\n",
        "        try:\n",
        "            with open(report_path, \"r\") as f:\n",
        "                report_data = json.load(f)\n",
        "\n",
        "            report_data[\"validation_score_IoU\"] = iou_score\n",
        "\n",
        "            with open(report_path, \"w\") as f:\n",
        "                json.dump(report_data, f, indent=2)\n",
        "\n",
        "            print(f\"✅ JSON report updated with IoU score.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️ Warning: Could not update JSON report. {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z2xZ4uIQpI0v",
        "outputId": "0dfae668-da51-44f1-8d2d-a8f8cbf2dd23"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Starting Stage 5: Benchmark Validation\n",
            "⚠️ Benchmark file not found at: \n",
            "   Skipping evaluation. Your output is saved at: /content/sam2_output_overlay.png\n"
          ]
        }
      ]
    }
  ]
}